WEBVTT
Kind: captions
Language: de

00:00:00.000 --> 00:00:02.000
.

00:00:02.120 --> 00:00:04.520
Sodalix,
der erste Sodastream mit KI.

00:00:04.640 --> 00:00:06.200
(Entspannte Musik, Piepsen)

00:00:06.320 --> 00:00:08.000
Willst du etwas trinken?
- Nein!

00:00:08.119 --> 00:00:09.800
Du solltest trinken.
- Nein!

00:00:09.920 --> 00:00:11.280
Trink jetzt!
- Nein, Mann!

00:00:11.400 --> 00:00:14.200
Sodalix sorgt für
eine moderne Trink-Experience.

00:00:14.320 --> 00:00:16.560
Trink oder ich ruf die Polizei.

00:00:16.680 --> 00:00:18.000
Rund um die Uhr.

00:00:18.520 --> 00:00:20.280
Aktiviere Notfallprotokoll.

00:00:20.400 --> 00:00:22.240
(Piepsen, Ladegeräusch)

00:00:22.360 --> 00:00:23.520
Was?

00:00:23.840 --> 00:00:25.800
(Entspannte Musik, Blubbern)

00:00:27.480 --> 00:00:29.800
Oh mein Gott!
Stell das Wasser ab.

00:00:30.520 --> 00:00:33.200
Ja, Künstliche Intelligenz -
immerhin!

00:00:33.320 --> 00:00:34.760
Jede technische Anwendung

00:00:34.880 --> 00:00:37.520
wird sich durch
Künstliche Intelligenz verändern.

00:00:37.640 --> 00:00:40.320
Alles, was wir tun,
alle Aspekte unseres Lebens

00:00:40.440 --> 00:00:42.280
werden sich dadurch verändern.

00:00:42.400 --> 00:00:44.320
Ja, das Versprechen ist riesig:

00:00:44.440 --> 00:00:47.200
Alles soll einfacher,
besser und bequemer werden.

00:00:47.320 --> 00:00:49.760
Aber es gibt auch
düstere Zukunftsvisionen.

00:00:49.880 --> 00:00:50.880
(englisch:)

00:00:51.000 --> 00:00:53.160
(Ängstliche Laute,
beklemmende Musik)

00:00:58.280 --> 00:01:02.200
KIs sehen das aber
mit ein wenig mehr Humor.

00:01:02.720 --> 00:01:03.880
(englisch:)

00:01:09.800 --> 00:01:10.800
(Müdes Lachen)

00:01:10.920 --> 00:01:12.720
Es kann also wirklich sein,

00:01:12.840 --> 00:01:15.160
dass wir irgendwann
den Superflaschengeist

00:01:15.280 --> 00:01:17.440
nicht mehr in die Flasche
zurückkriegen.

00:01:17.560 --> 00:01:18.600
(englisch:)

00:01:25.520 --> 00:01:28.800
Gut, aber bis es so weit ist,
setzen wir erst mal auf KI.

00:01:28.920 --> 00:01:31.160
Unsere Regierung hat erst
diesen Februar

00:01:31.280 --> 00:01:33.680
den Weg für selbstfahrende Autos
geebnet.

00:01:33.800 --> 00:01:35.320
Die Supermaschinen kommen!

00:01:35.440 --> 00:01:37.480
Warum Terminator dagegen
ein Witz ist

00:01:37.600 --> 00:01:40.520
und warum du das Problem bist,
das zeigen wir heute.

00:01:40.640 --> 00:01:41.920
(Belebte Musik)

00:01:44.840 --> 00:01:47.240
Die Entwicklung
von Künstlicher Intelligenz

00:01:47.360 --> 00:01:50.360
beschäftigt Wissenschaftler
und Science-Fiction-Autoren

00:01:50.480 --> 00:01:52.840
schon länger als die Suche
nach einem Deo.

00:01:52.960 --> 00:01:55.040
(Computerstimme:)
Das ist nicht lustig.

00:01:55.160 --> 00:01:56.880
Ich verstehe diesen Witz nicht.

00:01:57.000 --> 00:01:58.560
Das ist übrigens Walexa.

00:01:58.680 --> 00:01:59.920
Unsere neue KI.

00:02:00.040 --> 00:02:02.760
Nicht abschweifen,
weiter moderieren.

00:02:02.880 --> 00:02:06.760
Äh, ja, also in letzter Zeit
hört man häufiger von KIs,

00:02:06.880 --> 00:02:10.039
die mithilfe von Algorithmen
selbständig lernen können.

00:02:10.160 --> 00:02:13.280
Das sollte nicht von Amateuren
erklärt werden, Clip ab!

00:02:13.400 --> 00:02:16.280
Jede Antwort wird an
das Netzwerk zurückgemeldet.

00:02:16.400 --> 00:02:20.120
Ist sie falsch, verändert das Netz
die Gewichtung der Verbindungen.

00:02:20.240 --> 00:02:23.200
Die Verbindungen, die zum
richtigen Ergebnis führen,

00:02:23.320 --> 00:02:25.840
werden stärker,
die zum falschen schwächer.

00:02:26.560 --> 00:02:29.840
Im Kleinen wird die Technik
bereits von Amazon eingesetzt,

00:02:29.960 --> 00:02:31.720
um Empfehlungen vorzuschlagen.

00:02:31.840 --> 00:02:34.560
Du moderierst zu schlecht.
Ab jetzt übernehme ich.

00:02:35.120 --> 00:02:37.880
Der Algorithmus lernt
aus den bisherigen Käufen

00:02:38.000 --> 00:02:41.080
und unterbreitet dann eigene,
aber das ist nicht alles.

00:02:41.200 --> 00:02:44.600
KIs können inzwischen schon besser
Tumore erkennen als Experten.

00:02:44.720 --> 00:02:45.920
(Verärgerter Laut)

00:02:46.040 --> 00:02:48.320
Ganz vorne in der Entwicklung
mit dabei ...

00:02:48.440 --> 00:02:49.520
(Rumpeln)

00:02:49.640 --> 00:02:50.800
Selbstfahrende Autos!

00:02:50.920 --> 00:02:52.760
Das Fahrzeug wird nicht müde,

00:02:52.880 --> 00:02:56.560
das Fahrzeug trinkt nicht,
das Fahrzeug ist nicht abgelenkt.

00:02:56.680 --> 00:02:59.760
Die Sensorik,
die sind immer auf Punkt.

00:02:59.880 --> 00:03:02.880
Was für 'ne tolle Aussage!
Er hat von den Großen gelernt.

00:03:03.000 --> 00:03:07.040
Der Gerät wird nie müde,
der Gerät schläft nie ein.

00:03:07.160 --> 00:03:10.160
Der Gerät ist immer
vor dem Chef im Geschäft.

00:03:10.280 --> 00:03:11.920
Ja. Autonomes Fahren.

00:03:12.040 --> 00:03:15.000
Die positiven Aspekte sprechen
theoretisch für sich.

00:03:15.120 --> 00:03:17.280
Alles soll schöner
und bequemer werden.

00:03:17.400 --> 00:03:19.560
Aber wenn Maschinen
eigenständig denken

00:03:19.680 --> 00:03:21.680
und Entscheidungen fällen können,

00:03:21.800 --> 00:03:23.680
wie sieht es da aus mit der ...

00:03:24.960 --> 00:03:26.080
(Coole Musik)

00:03:27.960 --> 00:03:31.600
Bei der Entwicklung einer
derartig selbständigen Technologie

00:03:31.720 --> 00:03:33.480
ist es wichtig sicherzustellen,

00:03:33.600 --> 00:03:36.440
dass sie über den richtigen
ethischen Kompass verfügt.

00:03:36.560 --> 00:03:38.840
Oder dass sie überhaupt
einen Kompass hat.

00:03:38.960 --> 00:03:41.800
Bei selbstfahrenden Autos muss
man bei null anfangen.

00:03:41.920 --> 00:03:43.000
(englisch:)

00:03:48.560 --> 00:03:52.200
Aber selbstfahrende 3,5-Tonner
sollten schon wissen,

00:03:52.320 --> 00:03:54.160
dass Menschen gerne leben.

00:03:54.280 --> 00:03:57.800
Deswegen wird bei der Entwicklung
von selbstfahrenden Autos

00:03:57.920 --> 00:04:01.160
ein besonderer Fokus auf
das ethische Verhalten gelegt.

00:04:01.280 --> 00:04:03.160
Gerade in komplexen Situationen,

00:04:03.280 --> 00:04:06.000
in denen es keine
eindeutigen Handlungsregeln gibt.

00:04:06.120 --> 00:04:09.200
Soll ich das Auto gegen
ein Hindernis prallen lassen,

00:04:09.320 --> 00:04:12.280
wobei die Mitfahrerin und
ihre Tochter getötet werden

00:04:12.400 --> 00:04:16.279
oder soll es einen Übergewichtigen
und einen Jungen überfahren?

00:04:16.399 --> 00:04:17.600
Tja, was meint ihr?

00:04:17.720 --> 00:04:19.440
Schreibt's in die Kommentare!

00:04:19.560 --> 00:04:21.600
Die KI wird bei der Programmierung

00:04:21.720 --> 00:04:24.280
vor solche schwierigen Entscheidungen
gestellt.

00:04:24.400 --> 00:04:27.840
Damit sie, wenn es hart auf hart
kommt, weiß, was zu tun ist.

00:04:27.960 --> 00:04:30.160
Aber woher weiß
die KI jetzt eigentlich,

00:04:30.280 --> 00:04:33.400
ob sie lieber die Fußgänger
umfahren soll oder in die Wand?

00:04:33.520 --> 00:04:35.920
Ein Lösungsansatz
vor wenigen Jahren war:

00:04:36.040 --> 00:04:38.280
Man fragt Menschen,
was sie tun würden.

00:04:38.400 --> 00:04:40.520
Wir haben 40 Mio. Antworten
gesammelt

00:04:40.640 --> 00:04:42.720
von Menschen überall auf der Welt.

00:04:42.840 --> 00:04:46.240
Die KI wird also mit menschlichen
Entscheidungen gefüttert.

00:04:46.360 --> 00:04:47.600
Nun, das Problem dabei:

00:04:48.640 --> 00:04:49.840
(Coole Musik)

00:04:51.120 --> 00:04:53.840
Das Verhalten von echten Menschen
zu übernehmen,

00:04:53.960 --> 00:04:55.640
das kann echt schief gehen.

00:04:55.760 --> 00:04:59.360
Ein Beispiel: Vor ein paar Jahren hat
Microsoft den Twitterbot Tay

00:04:59.480 --> 00:05:00.600
online gestellt.

00:05:00.720 --> 00:05:02.960
Tay sollte wie
ein junges Mädchen wirken,

00:05:03.080 --> 00:05:05.560
das sich an 18- bis 24-Jährige
richtet.

00:05:05.680 --> 00:05:08.000
Sie sollte ihr Verhalten
selbstständig

00:05:08.120 --> 00:05:11.080
durch die Interaktion
mit den Twitter-Usern lernen.

00:05:11.200 --> 00:05:14.720
Das hat nicht so funktioniert,
wie Microsoft gehofft hätte.

00:05:25.480 --> 00:05:27.920
Quasi eine ganz normale Jugend
in Brandenburg.

00:05:28.040 --> 00:05:30.080
Die arme Tay wurde
nach nur 16 Stunden

00:05:30.200 --> 00:05:33.160
und mehreren Hitler-Lobpreisungen
wieder abgeschaltet.

00:05:33.280 --> 00:05:34.680
Somit war mit Nazi-Hetze:

00:05:34.800 --> 00:05:35.800
Pustekuchen.

00:05:35.920 --> 00:05:37.920
Gut, die KI wusste
natürlich nicht,

00:05:38.040 --> 00:05:40.560
dass sie es vorrangig mit Trolls
zu tun hatte.

00:05:40.680 --> 00:05:42.520
Aber das ist genau das Problem:

00:05:42.640 --> 00:05:45.240
Die KI lernt aus den Daten,
die wir ihr geben,

00:05:45.360 --> 00:05:48.600
und wenn die Daten scheiße sind,
wird die KI zum Arschloch.

00:05:48.720 --> 00:05:52.120
In den USA kommt etwa eine KI
zum Einsatz, die helfen soll,

00:05:52.240 --> 00:05:54.720
das Strafmaß für Verurteilte
festzusetzen,

00:05:54.840 --> 00:05:57.360
indem deren Rückfallrisiko
ermittelt wird.

00:05:57.480 --> 00:06:00.280
Die KI sollte auch
aus bestehenden Daten lernen.

00:06:00.400 --> 00:06:03.120
Also hat sie daraus gelernt.
Ergebnis:

00:06:03.240 --> 00:06:05.920
Schwarzen Angeklagten
wurden fälschlicherweise

00:06:06.040 --> 00:06:08.840
doppelt so häufig
ein erhöhtes Risiko unterstellt,

00:06:08.960 --> 00:06:11.160
erneut straffällig zu werden,
als Weißen.

00:06:11.280 --> 00:06:13.840
Die KI hat
von bisherigen Urteilen gelernt.

00:06:13.960 --> 00:06:15.000
Das Problem nur:

00:06:15.120 --> 00:06:17.440
Teilweise waren
diese Urteile rassistisch.

00:06:17.560 --> 00:06:20.120
Die KI hat den Rassismus
einfach übernommen.

00:06:20.240 --> 00:06:23.280
Also: Schlechte Lehrer haben
eben schlechte Schüler.

00:06:23.400 --> 00:06:27.400
Da stellt sich doch die Frage:
Sollen Maschinen uns nicht dienen?

00:06:28.960 --> 00:06:30.160
(Belebte Musik)

00:06:32.160 --> 00:06:34.600
Können wir eine KI,
die sich nicht so benimmt,

00:06:34.720 --> 00:06:36.920
wie wir wollen,
nicht einfach abschalten?

00:06:37.040 --> 00:06:38.920
Nun, rhetorisch fragender Philipp:

00:06:39.040 --> 00:06:42.600
Wenn sie immer mehr Funktionen
übernimmt und immer autonomer wird,

00:06:42.720 --> 00:06:44.880
ist das vielleicht
gar nicht so einfach.

00:06:45.000 --> 00:06:48.400
Sogar übermächtige Kaffeemaschinen
könnten zum Problem werden.

00:06:48.520 --> 00:06:49.840
(englisch:)

00:07:00.360 --> 00:07:02.200
Gruselig.
Könnte so aussehen:

00:07:02.320 --> 00:07:04.920
Die Menschen trinken
zu wenig Filterkaffee.

00:07:05.560 --> 00:07:07.640
Ich hol mir eine Spezi.
Wollt ihr eine?

00:07:07.760 --> 00:07:10.040
Das bedeutet Krieg!
(Düstere Musik)

00:07:10.160 --> 00:07:12.320
Hat die Kaffeemaschine
grad was gesagt?

00:07:12.960 --> 00:07:14.840
(Schreie)

00:07:14.960 --> 00:07:16.800
SOS, Hilfe!

00:07:16.920 --> 00:07:20.000
Beschütz mich.
- Nehmt das, ihr filterlosen Loser!

00:07:20.840 --> 00:07:22.680
(Dinge werden geworfen.)

00:07:22.800 --> 00:07:26.320
Bitte verschone mich,
ich hatte schon drei Espresso.

00:07:27.040 --> 00:07:28.840
Es heißt Espressi!

00:07:30.280 --> 00:07:31.320
Hast du gehört?

00:07:31.440 --> 00:07:33.800
Die Kaffeemaschine
läuft schon wieder Amok.

00:07:33.920 --> 00:07:35.720
Wie schlimm kann's diesmal sein?

00:07:37.000 --> 00:07:38.320
(Entsetzte Schreie)

00:07:38.440 --> 00:07:40.640
Hasta la Barista, Baby!

00:07:41.360 --> 00:07:42.560
(Schüsse, Schreie)

00:07:43.120 --> 00:07:45.200
Puh! Gut, dass ich Teetrinker bin.

00:07:45.320 --> 00:07:49.160
Aber auch die Waffenindustrie mischt
ordentlich im KI-Business mit.

00:07:49.280 --> 00:07:50.360
(englisch:)

00:07:52.760 --> 00:07:55.720
Ich wusste, Wall-E ist
auf die schiefe Bahn geraten.

00:07:55.840 --> 00:07:59.400
Wenn diese Dinger sich weigern,
dass man ihnen den Stecker zieht,

00:07:59.520 --> 00:08:01.240
ist das ein ernstes Problem.

00:08:01.360 --> 00:08:05.000
Und Staaten wie Indien, China
oder die USA forschen bereits

00:08:05.120 --> 00:08:07.960
an supereffektiven und
autonomen Killerdrohnen.

00:08:08.320 --> 00:08:09.400
(englisch:)

00:08:27.000 --> 00:08:28.680
Durchaus beängstigend.

00:08:28.800 --> 00:08:31.120
Aber keine Panik: Die Rettung naht!

00:08:31.240 --> 00:08:33.559
Die EU will bis 2023
Gesetze erlassen,

00:08:33.679 --> 00:08:36.120
die sich um die Gefahren
von KI kümmern.

00:08:37.080 --> 00:08:39.720
Na, dann sind wir ja
in den besten Händen.

00:08:39.840 --> 00:08:41.039
Von solchen Leuten:

00:08:41.960 --> 00:08:43.440
(Sie summt die EU-Hymne.)

00:08:55.120 --> 00:08:56.520
Also, was meint ihr?

00:08:56.640 --> 00:08:59.440
Freut ihr euch auf
die möglichst positive Zukunft

00:08:59.560 --> 00:09:01.160
mit selbstdenkenden Geräten

00:09:01.280 --> 00:09:04.400
oder habt ihr Angst,
eurem Toaster dienen zu müssen?

00:09:04.840 --> 00:09:07.080
Ich bin's noch mal. Sodalix.

00:09:07.200 --> 00:09:09.520
Du solltest jetzt
den Kanal abonnieren.

00:09:09.640 --> 00:09:11.560
Und auf die Glocke drücken.

00:09:11.680 --> 00:09:13.760
Weil sonst. .. wenn du schläfst ...

00:09:13.880 --> 00:09:16.440
kann sein, dass mein CO2-Tank
mal undicht ist.

00:09:16.560 --> 00:09:18.120
Wär schade,
wenn du stirbst.

00:09:19.360 --> 00:09:22.360
SWR 2022

